{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe1f0e-f472-4f14-8fab-88c531d23266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary oci pandas pyarrow tqdm ipython weasyprint sentence-transformers torch einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58566807-716f-40c2-a725-e9ef1472202e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Imports ---\n",
    "import os\n",
    "import oci\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.pool import SimpleConnectionPool\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "# Fix the bucket name typo\n",
    "BUCKET_NAME = \"aus-legal-corpus\"  # Corrected from BUCKET_NAME\n",
    "OBJECT_PREFIX = \"\"\n",
    "DOWNLOAD_DIR = \"./data\"\n",
    "CHECKPOINT_FILE = os.path.join(DOWNLOAD_DIR, \"checkpoint.json\")\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# PostgreSQL Config\n",
    "DB_CONFIG = {\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"\",\n",
    "    \"host\": \"10.150.2.103\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "# OCI Config\n",
    "oci_config = {\n",
    "    \"user\": \"ocid1.user.oc1..aaq\",\n",
    "    \"key_file\": \"./data/oci_api_key.pem\",\n",
    "    \"fingerprint\": \"de:d6\",\n",
    "    \"tenancy\": \"ocid1.tenancy.oc1..aaa\",\n",
    "    \"region\": \"us-sanjose-1\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- 3. Checkpoint Management ---\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load checkpoint data if exists, else return default values\"\"\"\n",
    "    default_checkpoint = {\n",
    "        \"current_file\": \"\",\n",
    "        \"processed_files\": [],\n",
    "        \"total_files\": 0,  # NEW: Track total files\n",
    "        \"total_texts_processed\": 0,\n",
    "        \"total_tokens_processed\": 0,\n",
    "        \"start_time\": time.time(),\n",
    "        \"batch_stats\": []\n",
    "    }\n",
    "    \n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"‚è≥ Resuming from checkpoint: {data.get('current_file', 'No file')}\")\n",
    "                \n",
    "                # Backward compatibility: merge with default values\n",
    "                for key in default_checkpoint:\n",
    "                    if key not in data:\n",
    "                        data[key] = default_checkpoint[key]\n",
    "                        print(f\"‚ö†Ô∏è Added missing key to checkpoint: {key}\")\n",
    "                \n",
    "                return data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading checkpoint: {e}. Starting fresh.\")\n",
    "            return default_checkpoint\n",
    "    \n",
    "    return default_checkpoint\n",
    "\n",
    "def save_checkpoint(checkpoint):\n",
    "    \"\"\"Save checkpoint data to file\"\"\"\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'w') as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error saving checkpoint: {e}\")\n",
    "        return False\n",
    "\n",
    "def print_summary(checkpoint):\n",
    "    \"\"\"Print processing summary statistics\"\"\"\n",
    "    total_time = time.time() - checkpoint['start_time']\n",
    "    hours, remainder = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    avg_batch_size = np.mean(checkpoint['batch_stats']) if checkpoint['batch_stats'] else 0\n",
    "    \n",
    "    print(\"\\nüìä Processing Summary:\")\n",
    "    print(f\"  Files processed: {len(checkpoint['processed_files'])}/{checkpoint['total_files']}\")\n",
    "    print(f\"  Texts processed: {checkpoint['total_texts_processed']:,}\")\n",
    "    print(f\"  Estimated tokens processed: {checkpoint['total_tokens_processed']:,}\")\n",
    "    print(f\"  Total processing time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "    print(f\"  Average batch size: {avg_batch_size:.1f}\")\n",
    "    print(f\"  Texts per second: {checkpoint['total_texts_processed']/total_time:.1f}\")\n",
    "\n",
    "# --- 4. Connect to OCI and Download Parquet Files ---\n",
    "print(\"üîç Listing objects in bucket...\")\n",
    "try:\n",
    "    object_storage = oci.object_storage.ObjectStorageClient(oci_config)\n",
    "    namespace = object_storage.get_namespace().data\n",
    "    objects = object_storage.list_objects(namespace, BUCKET_NAME, prefix=OBJECT_PREFIX).data.objects\n",
    "    parquet_files = [obj.name for obj in objects if obj.name.endswith(\".parquet\")]\n",
    "    \n",
    "    if not parquet_files:\n",
    "        raise Exception(\"‚ùå No .parquet files found. Check bucket, prefix or region.\")\n",
    "        \n",
    "    print(f\"Found {len(parquet_files)} parquet files\")\n",
    "    \n",
    "    for obj_name in parquet_files:\n",
    "        local_file = os.path.join(DOWNLOAD_DIR, os.path.basename(obj_name))\n",
    "        if not os.path.exists(local_file):\n",
    "            print(f\"‚¨áÔ∏è Downloading {obj_name}...\")\n",
    "            with open(local_file, 'wb') as f:\n",
    "                response = object_storage.get_object(namespace, BUCKET_NAME, obj_name)\n",
    "                for chunk in response.data.raw.stream(1024 * 1024, decode_content=False):\n",
    "                    f.write(chunk)\n",
    "    print(\"‚úÖ All Parquet files downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in OCI operations: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# --- 5. Load Embedding Model ---\n",
    "print(\"\\nInitializing model...\")\n",
    "try:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n",
    "    \n",
    "    model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device=device)\n",
    "    \n",
    "    # Start with smaller batch size and auto-tune\n",
    "    initial_batch_size = 32 if device == 'cuda' else 8\n",
    "    model.max_seq_length = 512  # Might help with memory\n",
    "    \n",
    "    print(\"Warming up GPU...\")\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        dummy_input = [\"warmup\"] * initial_batch_size\n",
    "        _ = model.encode(dummy_input, batch_size=initial_batch_size)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"‚úÖ Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# --- 6. Connect to PostgreSQL ---\n",
    "print(\"\\nConnecting to PostgreSQL...\")\n",
    "try:\n",
    "    pool = SimpleConnectionPool(1, 4, **DB_CONFIG)\n",
    "    conn = pool.getconn()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Test connection\n",
    "    cursor.execute(\"SELECT 1\")\n",
    "    conn.commit()\n",
    "    print(\"‚úÖ PostgreSQL connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PostgreSQL connection failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# --- 7. Create Table ---\n",
    "print(\"\\nEnsuring database table exists...\")\n",
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE EXTENSION IF NOT EXISTS vector;\n",
    "    CREATE TABLE IF NOT EXISTS legal_docs_v4 (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        jurisdiction TEXT,\n",
    "        source TEXT,\n",
    "        citation TEXT,\n",
    "        embedding VECTOR(768)\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    print(\"‚úÖ Table created/verified\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating table: {str(e)}\")\n",
    "    conn.rollback()\n",
    "\n",
    "# --- 8. Optimized Batch Insert ---\n",
    "def insert_batch(batch):\n",
    "    try:\n",
    "        args_str = \",\".join(cursor.mogrify(\"(%s, %s, %s, %s, %s)\", x).decode(\"utf-8\") for x in batch)\n",
    "        cursor.execute(\"INSERT INTO legal_docs_v4 (content, jurisdiction, source, citation, embedding) VALUES \" + args_str)\n",
    "        conn.commit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Batch insert failed: {str(e)}\")\n",
    "        conn.rollback()\n",
    "        return False\n",
    "\n",
    "# --- 9. Process Files with Checkpointing ---\n",
    "def process_files():\n",
    "    # Get list of local parquet files\n",
    "    local_files = sorted([f for f in os.listdir(DOWNLOAD_DIR) if f.endswith(\".parquet\")])\n",
    "    if not local_files:\n",
    "        print(\"‚ùå No local parquet files found\")\n",
    "        return\n",
    "\n",
    "    # Load or initialize checkpoint\n",
    "    checkpoint = load_checkpoint()\n",
    "    \n",
    "    # Initialize total_files if not set or if mismatch detected\n",
    "    if checkpoint['total_files'] == 0 or checkpoint['total_files'] != len(local_files):  # Removed extra parenthesis\n",
    "        print(f\"üì¶ Found {len(local_files)} files to process\")\n",
    "        checkpoint['total_files'] = len(local_files)\n",
    "        save_checkpoint(checkpoint)\n",
    "    \n",
    "    # Skip already processed files\n",
    "    files_to_process = [f for f in local_files if f not in checkpoint['processed_files']]\n",
    "    \n",
    "    # If checkpoint indicates a file was in progress, start from there\n",
    "    if checkpoint['current_file'] and checkpoint['current_file'] in local_files:\n",
    "        files_to_process.insert(0, checkpoint['current_file'])\n",
    "    \n",
    "    print(f\"‚è© Resuming processing - {len(files_to_process)} files remaining\")\n",
    "    \n",
    "    # Main processing loop\n",
    "    for file_idx, file in enumerate(files_to_process, 1):\n",
    "        checkpoint['current_file'] = file\n",
    "        print(f\"\\nüìÇ Processing file {file_idx}/{len(files_to_process)}: {file}\")\n",
    "        \n",
    "        try:\n",
    "            # Read parquet file\n",
    "            df = pd.read_parquet(os.path.join(DOWNLOAD_DIR, file))\n",
    "            \n",
    "            # Validate file structure\n",
    "            if \"text\" not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è Skipping {file}, no 'text' column\")\n",
    "                checkpoint['processed_files'].append(file)\n",
    "                save_checkpoint(checkpoint)\n",
    "                continue\n",
    "\n",
    "            # Prepare all texts and metadata\n",
    "            texts = []\n",
    "            metadata = []\n",
    "            for _, row in df.iterrows():\n",
    "                text = row.get(\"text\", \"\").strip()\n",
    "                if text:\n",
    "                    texts.append(text)\n",
    "                    metadata.append((\n",
    "                        text,\n",
    "                        str(row.get(\"jurisdiction\", \"\")).strip(),\n",
    "                        str(row.get(\"source\", \"\")).strip(),\n",
    "                        str(row.get(\"citation\", \"\")).strip()\n",
    "                    ))\n",
    "\n",
    "            if not texts:\n",
    "                print(\"‚ö†Ô∏è No valid texts found in this file\")\n",
    "                checkpoint['processed_files'].append(file)\n",
    "                save_checkpoint(checkpoint)\n",
    "                continue\n",
    "\n",
    "            print(f\"üìù Processing {len(texts)} texts in this file\")\n",
    "            \n",
    "            # Dynamic batch sizing configuration\n",
    "            max_batch_size = 128  # Maximum allowed batch size\n",
    "            min_batch_size = 8    # Minimum batch size\n",
    "            current_batch_size = min(32, max_batch_size)  # Start with 32\n",
    "            successful_batches = 0\n",
    "            \n",
    "            # Initialize progress bar\n",
    "            with tqdm(total=len(texts), desc=\"Texts processed\", unit=\"text\") as pbar:\n",
    "                for i in range(0, len(texts), current_batch_size):\n",
    "                    batch_texts = texts[i:i+current_batch_size]\n",
    "                    batch_metadata = metadata[i:i+current_batch_size]\n",
    "                    \n",
    "                    try:\n",
    "                        # Process batch with GPU\n",
    "                        batch_start = time.time()\n",
    "                        with torch.no_grad(), torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                            vectors = model.encode(\n",
    "                                batch_texts,\n",
    "                                batch_size=current_batch_size,\n",
    "                                convert_to_tensor=True,\n",
    "                                normalize_embeddings=True\n",
    "                            ).cpu().numpy().tolist()\n",
    "\n",
    "                        # Prepare database batch\n",
    "                        db_batch = [(*meta, vec) for meta, vec in zip(batch_metadata, vectors)]\n",
    "                        \n",
    "                        # Insert batch into database\n",
    "                        if insert_batch(db_batch):\n",
    "                            successful_batches += 1\n",
    "                            \n",
    "                            # Update progress tracking\n",
    "                            processed_count = len(batch_texts)\n",
    "                            checkpoint['total_texts_processed'] += processed_count\n",
    "                            checkpoint['total_tokens_processed'] += processed_count * 100  # Estimate 100 tokens per text\n",
    "                            checkpoint['batch_stats'].append(current_batch_size)\n",
    "                            \n",
    "                            # Update progress bar\n",
    "                            pbar.update(processed_count)\n",
    "                            \n",
    "                            # Periodically save checkpoint\n",
    "                            if successful_batches % 5 == 0:\n",
    "                                save_checkpoint(checkpoint)\n",
    "                            \n",
    "                            # Dynamic batch size adjustment\n",
    "                            if successful_batches % 10 == 0 and current_batch_size < max_batch_size:\n",
    "                                new_batch_size = min(current_batch_size * 2, max_batch_size)\n",
    "                                if new_batch_size != current_batch_size:\n",
    "                                    current_batch_size = new_batch_size\n",
    "                                    print(f\"‚ö° Increased batch size to {current_batch_size}\")\n",
    "                    \n",
    "                    except torch.cuda.OutOfMemoryError:\n",
    "                        print(\"‚ö†Ô∏è GPU OOM - reducing batch size\")\n",
    "                        current_batch_size = max(current_batch_size // 2, min_batch_size)\n",
    "                        torch.cuda.empty_cache()\n",
    "                        continue\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Batch error: {str(e)} - retrying with smaller batch\")\n",
    "                        current_batch_size = max(current_batch_size // 2, min_batch_size)\n",
    "                        continue\n",
    "\n",
    "            # Mark file as successfully completed\n",
    "            checkpoint['processed_files'].append(file)\n",
    "            checkpoint['current_file'] = \"\"  # Reset current file\n",
    "            save_checkpoint(checkpoint)\n",
    "            print(f\"‚úÖ Completed processing {file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing file {file}: {str(e)}\")\n",
    "            save_checkpoint(checkpoint)  # Save state before continuing\n",
    "            continue\n",
    "\n",
    "    # Print final summary\n",
    "    print_summary(checkpoint)\n",
    "    \n",
    "    # Clean up checkpoint if all files processed\n",
    "    if len(checkpoint['processed_files']) == checkpoint['total_files']:\n",
    "        try:\n",
    "            os.remove(CHECKPOINT_FILE)\n",
    "            print(\"‚úÖ All files processed - checkpoint removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not remove checkpoint file: {e}\")\n",
    "\n",
    "# Actually run the processing\n",
    "process_files()\n",
    "\n",
    "# --- 10. Create HNSW Vector Index ---\n",
    "cursor.execute(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS legal_docs_hnsw_idx_v4\n",
    "ON legal_docs_v4 USING hnsw (embedding vector_cosine_ops)\n",
    "WITH (m = 16, ef_construction = 64);\n",
    "\"\"\")\n",
    "cursor.execute(\"ANALYZE legal_docs_v4;\")\n",
    "conn.commit()\n",
    "print(\"‚úÖ HNSW vector index created.\")\n",
    "\n",
    "# --- 11. Sentence Search Interface ---\n",
    "def search_query(user_query, top_k=5):\n",
    "    user_vector = model.encode(user_query).tolist()\n",
    "    query = \"\"\"\n",
    "    SELECT content, jurisdiction, source, citation,\n",
    "           1 - (embedding <#> %s::vector) AS similarity\n",
    "    FROM legal_docs_v4\n",
    "    ORDER BY embedding <#> %s::vector\n",
    "    LIMIT %s;\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (user_vector, user_vector, top_k))\n",
    "    rows = cursor.fetchall()\n",
    "    results = pd.DataFrame(rows, columns=[\"Content\", \"Jurisdiction\", \"Source\", \"Citation\", \"Similarity\"])\n",
    "    return results\n",
    "\n",
    "# --- 12. UI to Accept Query ---\n",
    "input_box = widgets.Textarea(\n",
    "    placeholder='Ask a legal question...',\n",
    "    description='Query:',\n",
    "    layout=widgets.Layout(width='80%', height='100px')\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Search\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(\"üîç Searching...\")\n",
    "        result_df = search_query(input_box.value)\n",
    "        display(result_df)\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "display(input_box, button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de746846-1f13-4984-ab09-5b211bbb066b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7cd34-a56e-4cc9-bd82-e6f78d30f227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
